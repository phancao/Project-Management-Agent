# Context Optimization Strategy

## Current vs Best Practices Comparison

### What Cursor/Copilot/Claude Do

| Feature | Current Implementation | Cursor/Copilot | Claude | Recommendation |
|---------|----------------------|----------------|---------|----------------|
| **Token Counting** | ‚úÖ tiktoken | ‚úÖ tiktoken | ‚úÖ Own tokenizer | Keep current |
| **Compression Strategy** | Simple sliding window | Hierarchical | Hierarchical + Semantic | Upgrade to hierarchical |
| **Tool Results** | Array compression | Smart summarization | Entity extraction | Enhance |
| **Message Importance** | Fixed (prefix/recent) | Dynamic scoring | Semantic relevance | Add scoring |
| **Summarization** | ‚ùå None (TODO) | LLM-based | LLM-based + caching | **Critical: Implement** |
| **Context Windows** | Single pass | Multi-tier | Progressive | Add tiers |
| **Caching** | ‚ùå None | Prompt caching | Prompt caching | Add for system prompts |

---

## Recommended Improvements (Priority Order)

### 1. **CRITICAL: Implement LLM-Based Summarization** üî¥

**Current Problem:**
```python
def _create_summary_message(self, messages: List[BaseMessage]) -> BaseMessage:
    # TODO: summary implementation  ‚Üê This is empty!
    pass
```

**What Cursor Does:**
- Summarizes old conversations while keeping recent context
- Uses fast model (GPT-3.5) for summarization
- Caches summaries to avoid re-summarization

**Recommended Implementation:**
```python
async def _create_summary_message(
    self, 
    messages: List[BaseMessage],
    summary_model: str = "gpt-3.5-turbo"  # Fast & cheap
) -> BaseMessage:
    """
    Create intelligent summary of messages.
    
    Strategy (from Claude/Cursor):
    1. Group messages by conversation topics
    2. Extract key facts, decisions, and actions
    3. Preserve user intent and context
    4. Drop verbose intermediate steps
    """
    # Group messages into logical chunks
    chunks = self._group_messages_by_topic(messages)
    
    summaries = []
    for chunk in chunks:
        # Summarize each chunk
        summary_prompt = f"""Summarize this conversation chunk concisely:
        - Keep: User intents, key findings, decisions, errors
        - Drop: Verbose details, intermediate tool outputs
        - Format: Bullet points, max 200 words

        Messages:
        {self._format_messages_for_summary(chunk)}
        """
        
        # Use fast model for summarization
        summary = await llm.ainvoke(summary_prompt)
        summaries.append(summary)
    
    # Combine summaries
    return SystemMessage(
        content=f"üìù Previous Conversation Summary:\n" + "\n\n".join(summaries),
        name="context_summary"
    )
```

**Impact:** Reduces 10K tokens ‚Üí 1-2K tokens while keeping context!

---

### 2. **Message Importance Scoring** üü°

**What Cursor/Copilot Do:**
- Score messages by semantic relevance to current query
- Always keep: User queries, final answers, errors
- Sometimes keep: Important tool results, key decisions
- Drop: Verbose intermediate steps, redundant info

**Recommended Implementation:**
```python
def _score_message_importance(
    self, 
    message: BaseMessage,
    current_query: str
) -> float:
    """
    Score message importance (0.0 - 1.0).
    
    Cursor's approach:
    - User queries: 1.0 (always keep)
    - Error messages: 0.9 (critical)
    - Final answers: 0.9 (critical)
    - Recent messages (last 5): 0.8 (context)
    - Tool results with data: 0.6 (important)
    - Intermediate steps: 0.3 (can summarize)
    - Verbose logs: 0.1 (can drop)
    """
    score = 0.5  # Base score
    
    # Rule-based scoring
    if isinstance(message, HumanMessage):
        score = 1.0  # User queries always important
    elif isinstance(message, SystemMessage):
        score = 0.9  # System prompts important
    elif isinstance(message, AIMessage):
        if "[ERROR]" in str(message.content):
            score = 0.9  # Errors are critical
        elif message.name == "reporter":
            score = 0.9  # Final reports important
        else:
            score = 0.6  # Regular AI responses
    elif isinstance(message, ToolMessage):
        content_len = len(str(message.content))
        if content_len < 500:
            score = 0.7  # Small tool results important
        else:
            score = 0.4  # Large results can compress
    
    # Boost recent messages
    # (implement message age tracking)
    
    # Semantic similarity to current query
    # (optional: use embeddings)
    
    return score
```

**Strategy:**
```python
def _compress_by_importance(
    self,
    messages: List[BaseMessage],
    current_query: str
) -> List[BaseMessage]:
    """
    Compress messages based on importance scoring.
    
    Cursor's strategy:
    1. Score all messages
    2. Keep high-importance messages (>0.7)
    3. Summarize medium-importance (0.4-0.7)
    4. Drop low-importance (<0.4)
    """
    scored = [(msg, self._score_message_importance(msg, current_query)) 
              for msg in messages]
    
    keep = [msg for msg, score in scored if score >= 0.7]
    summarize = [msg for msg, score in scored if 0.4 <= score < 0.7]
    # drop = [msg for msg, score in scored if score < 0.4]
    
    # Summarize medium-importance messages
    if summarize:
        summary = await self._create_summary_message(summarize)
        keep.insert(len(keep) // 2, summary)  # Insert in middle
    
    return keep
```

---

### 3. **Hierarchical Compression** üü°

**What Claude Does:**
- Level 1: Full messages (last 5-10)
- Level 2: Summarized messages (10-50 messages ago)
- Level 3: Super-summary (50+ messages ago)
- System prompts: Always full (with prompt caching)

**Recommended Implementation:**
```python
def _hierarchical_compress(
    self,
    messages: List[BaseMessage]
) -> List[BaseMessage]:
    """
    Multi-tier compression like Claude.
    
    Tiers:
    - Tier 1 (Recent): Keep full (last 10 messages)
    - Tier 2 (Middle): Summarize (10-50 messages ago)
    - Tier 3 (Old): Super-summarize (50+ messages ago)
    """
    if len(messages) <= 10:
        return messages  # All recent, no compression
    
    # Tier 1: Recent (full)
    recent = messages[-10:]
    
    # Tier 2 & 3: Older messages
    older = messages[:-10]
    
    if len(older) <= 40:
        # Tier 2: Single summary
        summary = await self._create_summary_message(older)
        return [summary] + recent
    else:
        # Tier 3: Multi-level summary
        old_old = older[:-40]
        middle = older[-40:]
        
        super_summary = await self._create_summary_message(old_old)
        summary = await self._create_summary_message(middle)
        
        return [super_summary, summary] + recent
```

---

### 4. **Tool Result Smart Compression** üü¢

**Current:** Array truncation (keep first 20 items)

**What Cursor Does:**
- **Extract entities** instead of raw data
- **Statistical summaries** for numerical data
- **Semantic deduplication** for repetitive results

**Enhanced Implementation:**
```python
def _compress_tool_result(self, tool_message: ToolMessage) -> ToolMessage:
    """
    Intelligent tool result compression.
    
    Cursor's approach:
    - For lists: Extract key stats + first/last items
    - For errors: Keep full error message
    - For data: Extract schema + sample rows
    """
    content = tool_message.content
    
    try:
        data = json.loads(content) if isinstance(content, str) else content
        
        # Strategy 1: List compression
        if isinstance(data, list) and len(data) > 20:
            compressed = {
                "_summary": {
                    "total_items": len(data),
                    "first_5": data[:5],
                    "last_5": data[-5:],
                    "sample": data[len(data)//2:len(data)//2+2],  # Middle sample
                },
                "_stats": self._extract_stats(data),
                "_schema": self._extract_schema(data[0]) if data else {}
            }
            tool_message.content = json.dumps(compressed)
        
        # Strategy 2: Large dict compression
        elif isinstance(data, dict) and len(json.dumps(data)) > 5000:
            compressed = {
                "_summary": self._extract_key_info(data),
                "_schema": {k: type(v).__name__ for k, v in data.items()},
                "_size": len(json.dumps(data))
            }
            tool_message.content = json.dumps(compressed)
        
    except Exception as e:
        logger.warning(f"Tool result compression failed: {e}")
    
    return tool_message

def _extract_stats(self, data: list) -> dict:
    """Extract statistical summary from list data."""
    if not data:
        return {}
    
    # For numerical data
    if all(isinstance(d, (int, float)) for d in data):
        return {
            "count": len(data),
            "min": min(data),
            "max": max(data),
            "avg": sum(data) / len(data)
        }
    
    # For object data
    if all(isinstance(d, dict) for d in data):
        # Extract common fields
        common_fields = set(data[0].keys()) if data else set()
        return {
            "count": len(data),
            "fields": list(common_fields),
            "types": {k: type(data[0].get(k)).__name__ for k in common_fields}
        }
    
    return {"count": len(data)}
```

---

### 5. **Prompt Caching** üü¢

**What Claude/Cursor Do:**
- Cache system prompts (never change)
- Cache conversation summaries
- Cache tool schemas

**Implementation:**
```python
class ContextManager:
    def __init__(self, token_limit: int, ...):
        self.token_limit = token_limit
        self.cached_system_prompt_hash = None  # Cache key
        self.summary_cache = {}  # message_ids ‚Üí summary
    
    def compress_with_caching(self, messages: List[BaseMessage]) -> List[BaseMessage]:
        """
        Compress messages using caching.
        
        Claude's approach:
        - System prompt: Cache with long TTL
        - Summaries: Cache by content hash
        - Recent messages: No caching (always fresh)
        """
        system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
        other_msgs = [m for m in messages if not isinstance(m, SystemMessage)]
        
        # Use Claude's prompt caching API
        # (requires Claude API or implement local cache)
        for sys_msg in system_msgs:
            sys_msg.additional_kwargs = {
                **sys_msg.additional_kwargs,
                "cache_control": {"type": "ephemeral"}  # Claude caching
            }
        
        # Compress non-system messages
        compressed_other = self._compress_messages(other_msgs)
        
        return system_msgs + compressed_other
```

---

### 6. **Context Window Management** üü°

**What Cursor Does:**
- Different strategies for different agent types:
  - **Planner**: Keep full context (needs big picture)
  - **Coder**: Keep recent code + error messages
  - **Reporter**: Keep summaries + final results
  - **ReAct**: Keep last 5 turns only

**Implementation:**
```python
AGENT_CONTEXT_STRATEGIES = {
    "planner": {
        "preserve_prefix": 5,
        "max_tokens": 12000,
        "compression_mode": "hierarchical",
        "keep_tool_results": True
    },
    "coder": {
        "preserve_prefix": 3,
        "max_tokens": 8000,
        "compression_mode": "recent_only",
        "keep_tool_results": False  # Code doesn't need PM data
    },
    "reporter": {
        "preserve_prefix": 2,
        "max_tokens": 14000,
        "compression_mode": "summary_heavy",
        "keep_tool_results": True  # Needs data for report
    },
    "react_agent": {
        "preserve_prefix": 1,
        "max_tokens": 6000,
        "compression_mode": "minimal",
        "keep_tool_results": "last_3_only"
    }
}

def get_context_manager_for_agent(agent_type: str) -> ContextManager:
    """Get optimized context manager for agent type."""
    config = AGENT_CONTEXT_STRATEGIES.get(agent_type, AGENT_CONTEXT_STRATEGIES["planner"])
    
    return ContextManager(
        token_limit=config["max_tokens"],
        preserve_prefix_message_count=config["preserve_prefix"],
        compression_mode=config["compression_mode"]
    )
```

---

## Implementation Priority

### Phase 1: Quick Wins (1-2 days) üî¥
1. ‚úÖ **Implement `_create_summary_message`** with LLM (GPT-3.5-turbo)
2. ‚úÖ **Add message importance scoring** (rule-based)
3. ‚úÖ **Agent-specific context limits** (different limits per agent type)

### Phase 2: Enhanced Compression (3-5 days) üü°
4. ‚úÖ **Hierarchical compression** (3-tier system)
5. ‚úÖ **Smart tool result compression** (entity extraction, stats)
6. ‚úÖ **Context caching** (system prompts, summaries)

### Phase 3: Advanced (1-2 weeks) üü¢
7. ‚≠ï **Semantic similarity** (use embeddings for relevance)
8. ‚≠ï **Adaptive compression** (adjust based on query complexity)
9. ‚≠ï **Streaming context** (send context progressively)

---

## Expected Impact

| Metric | Current | After Phase 1 | After Phase 2 |
|--------|---------|---------------|---------------|
| **Context Size** | 22K tokens | ~8K tokens | ~4K tokens |
| **Reporter Success Rate** | 40% (fails on overflow) | 90% | 98% |
| **Context Retention** | Low (drops middle) | Medium (smart summary) | High (hierarchical) |
| **Cost per Query** | $0.03 | $0.015 | $0.01 |
| **Latency** | 30s+ | 25s | 20s |

---

## Comparison with Industry Leaders

### Cursor's Strategy
```
System Prompt (cached) ‚Üí [Summary of old context] ‚Üí [Last 10 messages] ‚Üí [Current query]
                ‚Üë                    ‚Üë                      ‚Üë
           Never changes      Summarized every      Always full detail
                              50 messages
```

### Claude's Strategy
```
[System + Instructions (cached)] ‚Üí [Conversation Summary] ‚Üí [Recent turns (full)] ‚Üí [Query]
            ‚Üë                              ‚Üë                        ‚Üë
    Prompt caching saves 90%      Multi-level summaries    Last 5-10 turns
```

### Our Recommended Strategy
```
[System (cached)] ‚Üí [Tier 3: Super-summary] ‚Üí [Tier 2: Summary] ‚Üí [Tier 1: Recent 10] ‚Üí [Query]
      ‚Üë                     ‚Üë                        ‚Üë                    ‚Üë
 Always keep         50+ msgs ago (200 tokens)   10-50 msgs (800 tokens)  Full (4K tokens)
```

---

## Code Changes Needed

### 1. Update `ContextManager.__init__`
```python
def __init__(
    self, 
    token_limit: int,
    preserve_prefix_message_count: int = 0,
    compression_mode: str = "hierarchical",  # NEW
    agent_type: str = "default",  # NEW
    summary_model: str = "gpt-3.5-turbo"  # NEW
):
    self.compression_mode = compression_mode
    self.agent_type = agent_type
    self.summary_model = summary_model
    self.summary_cache = {}
```

### 2. Implement `_create_summary_message` (CRITICAL!)
```python
async def _create_summary_message(
    self, 
    messages: List[BaseMessage]
) -> BaseMessage:
    # Use GPT-3.5-turbo for fast, cheap summarization
    from src.llms.llm import get_llm_by_type
    
    llm = get_llm_by_type("basic")  # Fast model
    
    summary_prompt = f"""...summarization prompt..."""
    summary = await llm.ainvoke(summary_prompt)
    
    return SystemMessage(
        content=f"üìù Context Summary:\\n{summary}",
        name="context_summary"
    )
```

### 3. Add importance-based compression
```python
def _compress_by_importance(self, messages, query):
    # Score, keep important, summarize medium, drop low
    pass
```

---

## Testing Strategy

### Test Case 1: Long PM Analysis
```python
# Scenario: Analyze 5 sprints with 200+ tasks
# Current: 22K tokens ‚Üí FAILS
# After Phase 1: 8K tokens ‚Üí SUCCESS
# After Phase 2: 4K tokens ‚Üí SUCCESS (better quality)
```

### Test Case 2: Multi-turn Conversation
```python
# Scenario: 20-turn conversation about sprint planning
# Current: Drops important middle context
# After: Keeps all important decisions via summaries
```

### Test Case 3: Error Recovery
```python
# Scenario: Agent encounters error, needs to retry
# Current: Full error context repeated
# After: Compressed error summary, more space for retry
```

---

## Next Steps

1. **Immediate**: Implement `_create_summary_message` (fixes current issue)
2. **This week**: Add importance scoring + hierarchical compression
3. **Next week**: Smart tool result compression + caching
4. **Future**: Semantic similarity + adaptive strategies

Would you like me to implement Phase 1 now to fix the current context overflow issue?


