# Automatic Escalation to Full Pipeline âœ…

## Problem Fixed

**User Issue:** "Even if it fallback to full analytic, it still failed why?"

**Root Cause:** **It NEVER fallbacked!** When ReAct succeeded but data was too large, the system had NO automatic escalation mechanism.

---

## The Fix: Smart Data Size Detection

### Before (Broken Flow)

```
User: "analyse sprint 5"
    â†“
[COORDINATOR] â†’ ReAct (fast path)
    â†“
[REACT-AGENT] âœ… Calls PM tools
               âœ… Gets ALL data at once (21,492 tokens)
               âœ… "Success!" â†’ Routes to REPORTER
    â†“
[REPORTER] âŒ Context overflow!
           âŒ "Data too large (21,492 vs 16,385 limit)"
           âŒ Shows error to user
    â†“
[END] â† Game over, no retry! âŒ
```

**Result:** User sees error, no solution

---

### After (Fixed Flow)

```
User: "analyse sprint 5"
    â†“
[COORDINATOR] â†’ ReAct (fast path)
    â†“
[REACT-AGENT] âœ… Calls PM tools
               âœ… Gets data (21,492 tokens)
               âš ï¸  Detects: "Data too large for reporter!"
               â¬†ï¸  ESCALATE to full pipeline
    â†“
[PLANNER] ğŸ“‹ Creates multi-step plan:
          Step 1: Get sprint info
          Step 2: Get sprint report
          Step 3: Get burndown data
    â†“
[PM_AGENT] Executes Step 1 â†’ Gets data
    â†“
[VALIDATOR] âœ… Validates + Compresses
    â†“
[PM_AGENT] Executes Step 2 â†’ Gets data
    â†“
[VALIDATOR] âœ… Validates + Compresses
    â†“
... (incremental processing)
    â†“
[REPORTER] âœ… Generates report with compressed data
    â†“
[END] âœ… Success! Full analysis delivered
```

**Result:** User gets complete analysis (takes longer but works!)

---

## What Changed

### New Trigger in ReAct: Data Size Check

**File:** `src/graph/nodes.py` (react_agent_node, line ~3105)

```python
# Trigger 4: Data too large for reporter (NEW!)
from src.utils.context_manager import ContextManager

# Count tokens in the ReAct output
output_tokens = ContextManager._count_tokens_with_tiktoken([
    {"role": "assistant", "content": output}
])

# Count tokens in current state (rough estimate)
state_messages = state.get("messages", [])
state_tokens = sum(
    len(str(msg.content)) // 4  # 1 token â‰ˆ 4 chars
    for msg in state_messages[-5:]  # Last 5 messages
)

total_estimated_tokens = output_tokens + state_tokens

# Get reporter's token limit (85% of model's context)
model_limit = get_llm_token_limit_by_type("basic") or 16385
reporter_limit = int(model_limit * 0.85)  # Reporter uses 85%

# If data is too large, escalate!
if total_estimated_tokens > reporter_limit:
    logger.warning(
        f"[REACT-AGENT] â¬†ï¸ Data too large ({total_estimated_tokens} tokens > {reporter_limit} limit) - "
        "escalating to full pipeline for incremental processing"
    )
    return Command(
        update={
            "escalation_reason": f"data_too_large ({total_estimated_tokens} tokens vs {reporter_limit} limit)",
            "react_attempts": intermediate_steps,
            "partial_result": output,
            "goto": "planner"
        },
        goto="planner"
    )
```

---

## ReAct Escalation Triggers (Complete List)

ReAct now escalates to full pipeline in **4 scenarios**:

| Trigger | Condition | Reason |
|---------|-----------|--------|
| **1. Too many iterations** | `>= 8 iterations` | Agent is struggling |
| **2. Repeated errors** | `>= 3 errors` | Tools are failing |
| **3. Agent requests planning** | Output contains "requires detailed planning" | Complex task detected |
| **4. Data too large (NEW!)** | `tokens > 85% of reporter limit` | Prevent reporter overflow |

---

## Why Full Pipeline Handles Large Data Better

### ReAct (Fast Path)
- âš¡ One-shot execution
- âœ… Great for simple queries
- âŒ Gets ALL data at once
- âŒ No compression between steps
- âŒ If data is huge â†’ fails

### Full Pipeline (Comprehensive)
- ğŸ—ï¸ Multi-step execution
- âœ… Incremental data loading
- âœ… Validator compresses after EACH step
- âœ… Handles large datasets gracefully
- âœ… Retry logic per step

**Example: "analyse sprint 5"**

**ReAct approach:**
```
Call sprint_report() â†’ Get 21K tokens at once â†’ Send to reporter â†’ FAIL âŒ
```

**Full Pipeline approach:**
```
Step 1: list_sprints()        â†’ Get 500 tokens  â†’ Validate â†’ Compress
Step 2: sprint_report()       â†’ Get 8K tokens   â†’ Validate â†’ Compress
Step 3: burndown_chart()      â†’ Get 4K tokens   â†’ Validate â†’ Compress
Step 4: list_tasks_in_sprint()â†’ Get 6K tokens   â†’ Validate â†’ Compress
                                                  â†“
Reporter: Receives 12K tokens (compressed from 18.5K) â†’ SUCCESS âœ…
```

---

## Expected Behavior Now

### Try: "analyse sprint 5"

**Scenario 1: Small dataset (< 13K tokens)**
```
Flow: ReAct â†’ Reporter â†’ Success
Time: ~5-7 seconds
Result: âœ… Quick answer
```

**Scenario 2: Large dataset (> 13K tokens)**
```
Flow: ReAct â†’ Detects size â†’ Escalates â†’ Planner â†’ PM_Agent â†’ Validator â†’ Reporter â†’ Success
Time: ~20-30 seconds
Result: âœ… Full comprehensive report
Logs: "[REACT-AGENT] â¬†ï¸ Data too large (21492 tokens > 13927 limit) - escalating to full pipeline"
```

**Scenario 3: Very complex query**
```
Flow: ReAct â†’ Agent requests planning â†’ Escalates â†’ Full Pipeline
Time: ~30-40 seconds
Result: âœ… Multi-step analysis
```

---

## Test It! ğŸš€

**Try these queries:**

1. **Simple query (should use ReAct):**
   ```
   "How many tasks in sprint 5?"
   ```
   Expected: Fast path (5s), direct answer

2. **Large data query (should escalate):**
   ```
   "analyse sprint 5"
   ```
   Expected: Auto-escalate (25s), full report

3. **Complex query (should escalate):**
   ```
   "Compare sprint 4 and sprint 5 velocity and predict sprint 6"
   ```
   Expected: Auto-escalate (30s), comprehensive analysis

---

## Logs to Watch For

### Successful Escalation
```
[REACT-AGENT] ğŸš€ Starting fast ReAct agent
[REACT-AGENT] âœ… Loaded 15 PM tools + web_search
[REACT-AGENT] Token check: output=12450, state=2100, total=14550, reporter_limit=13927
[REACT-AGENT] â¬†ï¸ Data too large (14550 tokens > 13927 limit) - escalating to full pipeline
[COORDINATOR] ğŸ“Š Using full pipeline: escalation=data_too_large (14550 tokens vs 13927 limit)
[PLANNER] ğŸ“‹ Creating multi-step plan...
[PM_AGENT] Executing Step 1...
[VALIDATOR] âœ… Validation passed, data within limits
...
[REPORTER] âœ… Generated final report
```

### No Escalation Needed
```
[REACT-AGENT] ğŸš€ Starting fast ReAct agent
[REACT-AGENT] Token check: output=2450, state=800, total=3250, reporter_limit=13927
[REACT-AGENT] âœ… Success - returning answer (156 chars)
[REPORTER] âœ… Generated final report
```

---

## Summary

âœ… **Fixed:** ReAct now detects data size BEFORE routing to reporter
âœ… **Added:** Automatic escalation when data > 85% of reporter limit
âœ… **Result:** Full pipeline handles large datasets via incremental processing
âœ… **UX:** Users get results instead of errors (just takes longer)

**Key insight:** Fast path (ReAct) is optimistic but smart enough to escalate when needed. Full pipeline is the fallback for complex/large data scenarios.


